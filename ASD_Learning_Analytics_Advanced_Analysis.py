import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.tree import DecisionTreeRegressor
import scipy.stats as stats
from scipy.stats import pearsonr, spearmanr
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
import warnings
import random

# Advanced Analysis of Correlations and Complex Patterns
# Computational Scaffolding for Inclusive Education - ASD
# Investigation of specific correlations by profile and mediating variables

warnings.filterwarnings('ignore')

# Set random seed for reproducibility
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
random.seed(RANDOM_STATE)

# Configure style
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

print("=== ADVANCED CORRELATION ANALYSIS ===")
print("Computational Scaffolding for Inclusive Education")
print("Investigation of Complex Patterns - ASD")
print("=" * 60)

# Load data (assuming they were generated by the previous code with fixed seed)
try:
    students_df = pd.read_csv('students_asd_data.csv')
    sessions_df = pd.read_csv('learning_sessions_data.csv')
    analytics_df = pd.read_csv('learning_analytics_data.csv')
    print("âœ“ Data loaded successfully")
except FileNotFoundError:
    print("âŒ Error: Please run the synthetic data generator with a fixed seed first")
    students_df = None
    sessions_df = None
    analytics_df = None

# ---
### 1. SPECIFIC PROFILE CORRELATION ANALYSIS


print("\n1. SPECIFIC PROFILE CORRELATION ANALYSIS")
print("=" * 60)

def analyze_correlation_by_profile(df_students, df_sessions, df_analytics, profile_var, profile_values):
    """
    Analyzes correlations between engagement and progress for specific profiles
    """
    results = {}

    if df_students is None or df_sessions is None or df_analytics is None:
        print("   Insufficient data for correlation analysis by profile.")
        return results

    for value in profile_values:
        # Filter students by profile
        profile_students_ids = df_students[df_students[profile_var] == value]['student_id']

        # Filter analytics data for these students
        profile_analytics = df_analytics[df_analytics['student_id'].isin(profile_students_ids)].dropna(
            subset=['avg_engagement', 'avg_stress', 'zpd_advantage', 'skill_improvement']
        )

        if len(profile_analytics) > 1:  # Need at least 2 data points for reliable correlation analysis
            # Correlation engagement vs progress
            corr_engagement, p_engagement = pearsonr(
                profile_analytics['avg_engagement'],
                profile_analytics['skill_improvement']
            )

            # Correlation stress vs progress
            corr_stress, p_stress = pearsonr(
                profile_analytics['avg_stress'],
                profile_analytics['skill_improvement']
            )

            # Correlation ZPD advantage vs progress
            corr_zpd, p_zpd = pearsonr(
                profile_analytics['zpd_advantage'],
                profile_analytics['skill_improvement']
            )

            results[value] = {
                'n_students': len(profile_analytics),
                'engagement_corr': corr_engagement,
                'engagement_p': p_engagement,
                'stress_corr': corr_stress,
                'stress_p': p_stress,
                'zpd_corr': corr_zpd,
                'zpd_p': p_zpd,
                'avg_improvement': profile_analytics['skill_improvement'].mean()
            }
        else:
            # Set to NaN if not enough data, allowing later checks to skip printing
            results[value] = {
                'n_students': len(profile_analytics),
                'engagement_corr': np.nan,
                'engagement_p': np.nan,
                'stress_corr': np.nan,
                'stress_p': np.nan,
                'zpd_corr': np.nan,
                'zpd_p': np.nan,
                'avg_improvement': np.nan
            }
    return results

# Analysis by learning style
print("\nðŸ§  CORRELATIONS BY LEARNING STYLE:")
style_correlations = {}
if students_df is not None and sessions_df is not None and analytics_df is not None:
    learning_styles = students_df['preferred_learning_style'].unique()
    style_correlations = analyze_correlation_by_profile(
        students_df, sessions_df, analytics_df,
        'preferred_learning_style', learning_styles
    )

    for style, data in style_correlations.items():
        print(f"\n{style.upper()} (n={data['n_students']}):")
        if not np.isnan(data['engagement_corr']):
            print(f"   Engagement â†’ Progress: r={data['engagement_corr']:.3f} (p={data['engagement_p']:.3f})")
            print(f"   Stress â†’ Progress: r={data['stress_corr']:.3f} (p={data['stress_p']:.3f})")
            print(f"   ZPD â†’ Progress: r={data['zpd_corr']:.3f} (p={data['zpd_p']:.3f})")
            print(f"   Average Improvement: {data['avg_improvement']:.3f}")
        else:
            print("   Insufficient data for analysis.")
else:
    print("Student, session, or analytics data not loaded, skipping correlation analysis by profile.")


# Analysis by ASD severity
print("\nðŸ”„ CORRELATIONS BY ASD SEVERITY:")
severity_correlations = {}
if students_df is not None and sessions_df is not None and analytics_df is not None:
    asd_severities = students_df['asd_severity'].unique()
    severity_correlations = analyze_correlation_by_profile(
        students_df, sessions_df, analytics_df,
        'asd_severity', asd_severities
    )

    for severity, data in severity_correlations.items():
        print(f"\n{severity.upper()} (n={data['n_students']}):")
        if not np.isnan(data['engagement_corr']):
            print(f"   Engagement â†’ Progress: r={data['engagement_corr']:.3f} (p={data['engagement_p']:.3f})")
            print(f"   Stress â†’ Progress: r={data['stress_corr']:.3f} (p={data['stress_p']:.3f})")
            print(f"   ZPD â†’ Progress: r={data['zpd_corr']:.3f} (p={data['zpd_p']:.3f})")
            print(f"   Average Improvement: {data['avg_improvement']:.3f}")
        else:
            print("   Insufficient data for analysis.")
else:
    print("Student, session, or analytics data not loaded, skipping correlation analysis by severity.")


# Analysis by communication level
print("\nðŸ’¬ CORRELATIONS BY COMMUNICATION LEVEL:")
comm_correlations = {}
if students_df is not None and sessions_df is not None and analytics_df is not None:
    comm_levels = students_df['communication_level'].unique()
    comm_correlations = analyze_correlation_by_profile(
        students_df, sessions_df, analytics_df,
        'communication_level', comm_levels
    )

    for comm, data in comm_correlations.items():
        print(f"\n{comm.upper()} (n={data['n_students']}):")
        if not np.isnan(data['engagement_corr']):
            print(f"   Engagement â†’ Progress: r={data['engagement_corr']:.3f} (p={data['engagement_p']:.3f})")
            print(f"   Stress â†’ Progress: r={data['stress_corr']:.3f} (p={data['stress_p']:.3f})")
            print(f"   ZPD â†’ Progress: r={data['zpd_corr']:.3f} (p={data['zpd_p']:.3f})")
            print(f"   Average Improvement: {data['avg_improvement']:.3f}")
        else:
            print("   Insufficient data for analysis.")
else:
    print("Student, session, or analytics data not loaded, skipping correlation analysis by communication level.")

# ---
### 2. INTRODUCTION OF MEDIATING VARIABLES


print("\n2. INTRODUCTION OF MEDIATING VARIABLES")
print("=" * 60)

# Create mediating variables based on session data
def create_mediator_variables(df_sessions, df_students):
    """
    Creates mediating variables for causal analysis
    """
    mediator_data = []

    if df_students is None or df_sessions is None:
        print("   Insufficient data for creating mediating variables.")
        return pd.DataFrame()

    for student_id in df_students['student_id'].unique():
        student_sessions = df_sessions[df_sessions['student_id'] == student_id]

        if len(student_sessions) > 0:
            # Visual focus (based on visual scaffolding and performance)
            visual_sessions = student_sessions[student_sessions['scaffolding_type'] == 'visual_prompt']
            visual_focus = visual_sessions['performance_score'].mean() if len(visual_sessions) > 0 else 0.0

            # Time on task (average session duration)
            avg_task_time = student_sessions['session_duration_minutes'].mean()

            # Engagement consistency (variability)
            # Handle the case where std() might be NaN (e.g., single session)
            engagement_std = student_sessions['engagement_level'].std()
            engagement_consistency = 1 / (1 + engagement_std) if not pd.isna(engagement_std) else 0.0

            # Learning efficiency (progress per hour)
            total_time_hours = student_sessions['session_duration_minutes'].sum() / 60
            # Ensure skill levels are available before calculating efficiency
            if 'current_skill_level' in student_sessions.columns and len(student_sessions) > 1:
                skill_start = student_sessions['current_skill_level'].iloc[0]
                skill_end = student_sessions['current_skill_level'].iloc[-1]
                learning_efficiency = (skill_end - skill_start) / total_time_hours if total_time_hours > 0 else 0.0
            else:
                learning_efficiency = 0.0

            # Adaptability (variation in response to different scaffoldings)
            scaffolding_performance = student_sessions.groupby('scaffolding_type')['performance_score'].mean()
            # Handle std being NaN or 0 for adaptability
            if len(scaffolding_performance) > 1 and not pd.isna(scaffolding_performance.std()) and scaffolding_performance.std() != 0:
                adaptability = scaffolding_performance.std()
            else:
                adaptability = 0.0 # Default to 0 if no variation or insufficient data

            # Stress resistance (performance when stress is high)
            high_stress_sessions = student_sessions[student_sessions['stress_level'] > 0.7]
            stress_resistance = high_stress_sessions['performance_score'].mean() if len(high_stress_sessions) > 0 else 0.0

            mediator_data.append({
                'student_id': student_id,
                'visual_focus_score': visual_focus,
                'avg_task_time': avg_task_time,
                'engagement_consistency': engagement_consistency,
                'learning_efficiency': learning_efficiency,
                'adaptability_score': adaptability,
                'stress_resistance': stress_resistance
            })

    return pd.DataFrame(mediator_data)

# Generate mediating variables
extended_analytics = pd.DataFrame() # Initialize to empty DataFrame
if sessions_df is not None and students_df is not None and analytics_df is not None:
    mediators_df = create_mediator_variables(sessions_df, students_df)

    # Combine with analytics data
    # Use .copy() to avoid SettingWithCopyWarning later
    extended_analytics = analytics_df.merge(mediators_df, on='student_id', how='left').copy()
    extended_analytics = extended_analytics.merge(
        students_df[['student_id', 'preferred_learning_style', 'asd_severity', 'communication_level']],
        on='student_id', how='left'
    ).copy() # Ensure final merge also creates a copy

    print("âœ“ Mediating variables created:")
    print("   - Visual focus: Performance with visual prompts")
    print("   - Time on task: Average session duration")
    print("   - Engagement consistency: Engagement stability")
    print("   - Learning efficiency: Progress per hour")
    print("   - Adaptability: Variation in response to scaffoldings")
    print("   - Stress resistance: Performance under high stress")
else:
    print("Session, student, or analytics data not loaded, skipping mediator variable creation and further analysis.")

# ---
### 3. POLYNOMIAL REGRESSION ANALYSIS


print("\n3. POLYNOMIAL REGRESSION ANALYSIS")
print("=" * 60)

def polynomial_analysis(X, y, degrees=[1, 2, 3], feature_name="Feature"):
    """
    Tests different polynomial degrees to find the best fit
    """
    results = {}

    # Ensure X is a 2D array for sklearn
    if X.ndim == 1:
        X = X.reshape(-1, 1)

    if len(X) < 2:  # Need at least 2 data points for regression
        print(f"   Insufficient data for polynomial analysis of {feature_name}.")
        return results

    for degree in degrees:
        try:
            # Create polynomial features
            poly_features = PolynomialFeatures(degree=degree, include_bias=False)
            X_poly = poly_features.fit_transform(X)

            # Train model
            model = LinearRegression()
            model.fit(X_poly, y)

            # Predictions
            y_pred = model.predict(X_poly)

            # Metrics
            r2 = r2_score(y, y_pred)
            mse = mean_squared_error(y, y_pred)

            results[degree] = {
                'r2': r2,
                'mse': mse,
                'model': model,
                'poly_features': poly_features
            }
        except ValueError as e:
            print(f"   Error in polynomial analysis of degree {degree} for {feature_name}: {e}")
            results[degree] = {
                'r2': np.nan,
                'mse': np.nan,
                'model': None,
                'poly_features': None
            }
        except Exception as e: # Catch other potential errors during fitting
            print(f"   An unexpected error occurred in polynomial analysis of degree {degree} for {feature_name}: {e}")
            results[degree] = {
                'r2': np.nan,
                'mse': np.nan,
                'model': None,
                'poly_features': None
            }
    return results

# Polynomial analysis: Engagement vs Progress
print("\nðŸ“ˆ POLYNOMIAL REGRESSION: ENGAGEMENT â†’ PROGRESS")
engagement_poly = {} # Initialize
if not extended_analytics.empty and len(extended_analytics) > 1:
    # Drop NaN values from both columns to ensure alignment
    valid_data_eng = extended_analytics[['avg_engagement', 'skill_improvement']].dropna()
    if len(valid_data_eng) > 1:
        engagement_poly = polynomial_analysis(
            valid_data_eng['avg_engagement'].values,
            valid_data_eng['skill_improvement'].values,
            feature_name="Engagement"
        )
        for degree, results in engagement_poly.items():
            if not np.isnan(results['r2']):
                print(f"   Degree {degree}: RÂ² = {results['r2']:.4f}, MSE = {results['mse']:.4f}")
    else:
        print("   Insufficient valid data for polynomial analysis of engagement.")
else:
    print("Extended analytics data not available or insufficient, skipping polynomial analysis for engagement.")


# Polynomial analysis: Time on task vs Progress
print("\nâ±ï¸ POLYNOMIAL REGRESSION: TIME ON TASK â†’ PROGRESS")
task_time_poly = {} # Initialize
if not extended_analytics.empty and len(extended_analytics) > 1:
    # Ensure corresponding skill_improvement values are used
    valid_data_task_time = extended_analytics[['avg_task_time', 'skill_improvement']].dropna()
    if len(valid_data_task_time) > 1:
        task_time_poly = polynomial_analysis(
            valid_data_task_time['avg_task_time'].values,
            valid_data_task_time['skill_improvement'].values,
            feature_name="Task Time"
        )
        for degree, results in task_time_poly.items():
            if not np.isnan(results['r2']):
                print(f"   Degree {degree}: RÂ² = {results['r2']:.4f}, MSE = {results['mse']:.4f}")
    else:
        print("   Insufficient valid data for polynomial analysis of task time.")
else:
    print("Extended analytics data not available or insufficient, skipping polynomial analysis for task time.")


# ---
### 4. RANDOM FOREST ANALYSIS


print("\n4. RANDOM FOREST ANALYSIS")
print("=" * 60)

# Prepare data for Random Forest
def prepare_rf_data(df):
    """
    Prepares data for Random Forest with categorical variable encoding
    """
    feature_columns = [
        'avg_engagement', 'avg_stress', 'zpd_advantage', 'avg_completion_rate',
        'total_learning_time_hours', 'visual_focus_score', 'avg_task_time',
        'engagement_consistency', 'learning_efficiency', 'adaptability_score',
        'stress_resistance'
    ]
    categorical_columns = ['preferred_learning_style', 'asd_severity', 'communication_level']
    target_column = 'skill_improvement'

    # Drop rows with NaN in any of the relevant columns
    cols_to_check = feature_columns + categorical_columns + [target_column]
    df_clean = df.dropna(subset=cols_to_check).copy()

    if df_clean.empty:
        print("   Insufficient clean data for Random Forest.")
        return pd.DataFrame(), np.array([]), []

    X_numeric = df_clean[feature_columns]

    # Encoding categorical variables
    # Ensure all unique values from original students_df are considered for encoding consistency
    # This assumes students_df is always available when extended_analytics is not empty
    if students_df is not None:
        le_style = LabelEncoder()
        le_severity = LabelEncoder()
        le_comm = LabelEncoder()

        # Fit on all unique values from students_df to ensure consistent encoding
        le_style.fit(students_df['preferred_learning_style'].unique())
        le_severity.fit(students_df['asd_severity'].unique())
        le_comm.fit(students_df['communication_level'].unique())

        X_categorical = pd.DataFrame({
            'learning_style_encoded': le_style.transform(df_clean['preferred_learning_style']),
            'severity_encoded': le_severity.transform(df_clean['asd_severity']),
            'communication_encoded': le_comm.transform(df_clean['communication_level'])
        })
    else: # Fallback if students_df somehow isn't available, although unlikely given previous checks
        print("   Warning: students_df not available, categorical encoding might be inconsistent.")
        X_categorical = pd.DataFrame() # Empty if no student data

    X = pd.concat([X_numeric.reset_index(drop=True), X_categorical.reset_index(drop=True)], axis=1)
    y = df_clean[target_column].values

    return X, y, feature_columns + list(X_categorical.columns)


rf_r2 = np.nan
rf_mse = np.nan
feature_importance = pd.DataFrame()
X_train, X_test, y_train, y_test = pd.DataFrame(), np.array([]), np.array([]), np.array([]) # Initialize

if not extended_analytics.empty:
    X, y, feature_names = prepare_rf_data(extended_analytics)

    if not X.empty and len(y) > 1:
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)

        # Train Random Forest
        rf_model = RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE, max_depth=10)
        rf_model.fit(X_train, y_train)

        # Predictions
        y_pred = rf_model.predict(X_test)

        # Metrics
        rf_r2 = r2_score(y_test, y_pred)
        rf_mse = mean_squared_error(y_test, y_pred)

        print(f"ðŸŒ³ RANDOM FOREST RESULTS:")
        print(f"   RÂ² Score: {rf_r2:.4f}")
        print(f"   MSE: {rf_mse:.4f}")
        print(f"   Number of features: {len(feature_names)}")

        # Feature importance
        feature_importance = pd.DataFrame({
            'feature': feature_names,
            'importance': rf_model.feature_importances_
        }).sort_values('importance', ascending=False)

        print(f"\nðŸ” TOP 10 MOST IMPORTANT FEATURES:")
        for idx, row in feature_importance.head(10).iterrows():
            print(f"   {row['feature']}: {row['importance']:.4f}")
    else:
        print("   Insufficient data for Random Forest training.")
else:
    print("Extended analytics data not available, skipping Random Forest analysis.")


# ---
### 5. DECISION TREE ANALYSIS FOR INTERPRETABILITY


print("\n5. DECISION TREE ANALYSIS")
print("=" * 60)

dt_r2 = np.nan
dt_mse = np.nan

if not X_train.empty and len(y_train) > 1: # Check if X_train and y_train were successfully populated
    dt_model = DecisionTreeRegressor(max_depth=5, random_state=RANDOM_STATE)
    dt_model.fit(X_train, y_train)

    # Predictions
    dt_pred = dt_model.predict(X_test)
    dt_r2 = r2_score(y_test, dt_pred)
    dt_mse = mean_squared_error(y_test, dt_pred)

    print(f"ðŸŒ² DECISION TREE RESULTS:")
    print(f"   RÂ² Score: {dt_r2:.4f}")
    print(f"   MSE: {dt_mse:.4f}")

    # Tree rules (simplified)
    def extract_rules(tree, feature_names, max_rules=10):
        """
        Extracts interpretable rules from the decision tree
        """
        tree_rules = []

        def recurse(node, depth=0, condition=""):
            if depth > 3 or len(tree_rules) >= max_rules:  # Limit depth and number of rules
                return

            if tree.children_left[node] == tree.children_right[node]:  # Leaf node
                value = tree.value[node][0][0]
                samples = tree.n_node_samples[node]
                if samples > 0: # Only include rules for nodes with samples
                    tree_rules.append({
                        'condition': condition,
                        'prediction': value,
                        'samples': samples,
                        'depth': depth
                    })
            else: # Internal node
                feature_idx = tree.feature[node]
                if feature_idx >= 0 and feature_idx < len(feature_names): # Check if feature index is valid
                    feature = feature_names[feature_idx]
                    threshold = tree.threshold[node]

                    # Left branch
                    left_condition = f"{condition} AND {feature} <= {threshold:.3f}" if condition else f"{feature} <= {threshold:.3f}"
                    recurse(tree.children_left[node], depth + 1, left_condition)

                    # Right branch
                    right_condition = f"{condition} AND {feature} > {threshold:.3f}" if condition else f"{feature} > {threshold:.3f}"
                    recurse(tree.children_right[node], depth + 1, right_condition)
                # If feature_idx is invalid, this branch is effectively skipped as no new rules are added.

        # Check if the tree has nodes before starting recursion
        if tree.node_count > 0:
            recurse(0)
        else:
            print("   Decision tree has no nodes.")

        # Sort by absolute prediction to prioritize impact and then truncate
        return sorted(tree_rules, key=lambda x: abs(x['prediction']), reverse=True)[:max_rules]

    rules = extract_rules(dt_model.tree_, feature_names)

    print(f"\nðŸ“‹ MAIN TREE RULES:")
    if rules:
        for i, rule in enumerate(rules[:5]): # Only print top 5 for conciseness
            print(f"   {i+1}. IF {rule['condition']}")
            print(f"         THEN progress â‰ˆ {rule['prediction']:.3f} (n={rule['samples']})")
    else:
        print("   Could not extract interpretable rules.")

else:
    print("Training data for Decision Tree not available or insufficient, skipping Decision Tree analysis.")


# ---
### 6. ADVANCED VISUALIZATIONS


print("\n6. GENERATING ADVANCED VISUALIZATIONS")
print("=" * 60)

# Added a comprehensive check to ensure necessary dataframes and variables are defined and have data
# Before plotting, ensure all required components are valid
plotting_possible = (
    style_correlations and # Must have some style correlations processed
    severity_correlations and # Must have some severity correlations processed
    analytics_df is not None and
    not extended_analytics.empty and # extended_analytics must not be empty
    engagement_poly and # engagement_poly must have results
    2 in engagement_poly and # specifically check for degree 2 model
    engagement_poly[2]['model'] is not None and # model must be trained
    not feature_importance.empty and # feature_importance must be populated
    not np.isnan(rf_r2) and # RF R2 must be a valid number
    not np.isnan(dt_r2) # DT R2 must be a valid number
)

if plotting_possible:
    # Create figure with multiple subplots
    fig, axes = plt.subplots(3, 2, figsize=(16, 18))
    fig.suptitle('Advanced Analysis: Correlations and Complex Patterns - ASD', fontsize=16, fontweight='bold')

    # 1. Correlation heatmap by learning style
    ax1 = axes[0, 0]
    style_corr_data_for_heatmap = {
        style: [data['engagement_corr'], data['stress_corr'], data['zpd_corr']]
        for style, data in style_correlations.items() if not np.isnan(data['engagement_corr'])
    }
    if style_corr_data_for_heatmap:
        style_corr_matrix = pd.DataFrame(style_corr_data_for_heatmap, index=['Engagement', 'Stress', 'ZPD'])
        sns.heatmap(style_corr_matrix, annot=True, cmap='RdBu_r', center=0, ax=ax1, fmt=".2f") # Added fmt for better display
        ax1.set_title('Correlations by Learning Style')
    else:
        ax1.text(0.5, 0.5, 'Insufficient data for heatmap', horizontalalignment='center', verticalalignment='center', transform=ax1.transAxes)
        ax1.set_title('Correlations by Learning Style')
        ax1.axis('off')


    # 2. Progress boxplot by severity
    ax2 = axes[0, 1]
    severity_progress_list = []
    if students_df is not None and analytics_df is not None and 'asd_severity' in students_df.columns:
        # Merge relevant dataframes to get severity and skill improvement in one place
        temp_merged_df = students_df.merge(analytics_df, on='student_id', how='inner')
        if not temp_merged_df.empty and 'asd_severity' in temp_merged_df.columns and 'skill_improvement' in temp_merged_df.columns:
            sns.boxplot(data=temp_merged_df, x='asd_severity', y='skill_improvement', ax=ax2)
            ax2.set_title('Progress Distribution by ASD Severity')
            ax2.set_xlabel('ASD Severity')
            ax2.set_ylabel('Skill Improvement')
        else:
            ax2.text(0.5, 0.5, 'Insufficient data for boxplot', horizontalalignment='center', verticalalignment='center', transform=ax2.transAxes)
            ax2.set_title('Progress Distribution by ASD Severity')
            ax2.axis('off')
    else:
        ax2.text(0.5, 0.5, 'Insufficient data for boxplot', horizontalalignment='center', verticalalignment='center', transform=ax2.transAxes)
        ax2.set_title('Progress Distribution by ASD Severity')
        ax2.axis('off')

    # 3. Scatter plot with polynomial regression (Engagement vs Progress)
    ax3 = axes[1, 0]
    # Re-check valid_data_eng for plotting
    valid_data_eng_for_plot = extended_analytics[['avg_engagement', 'skill_improvement']].dropna()
    if not valid_data_eng_for_plot.empty and len(valid_data_eng_for_plot) > 1 and 2 in engagement_poly and engagement_poly[2]['model'] is not None:
        x_eng = valid_data_eng_for_plot['avg_engagement'].values
        y_prog = valid_data_eng_for_plot['skill_improvement'].values

        ax3.scatter(x_eng, y_prog, alpha=0.6, s=30)

        # Plot polynomial curve of degree 2
        best_poly = engagement_poly[2]
        x_smooth = np.linspace(x_eng.min(), x_eng.max(), 100).reshape(-1, 1) # Reshape for PolynomialFeatures
        X_smooth_poly = best_poly['poly_features'].transform(x_smooth)
        y_smooth = best_poly['model'].predict(X_smooth_poly)
        ax3.plot(x_smooth, y_smooth, color='red', linewidth=2, label=f'Polynomial Degree 2 (RÂ²={best_poly["r2"]:.3f})')
        ax3.set_xlabel('Average Engagement')
        ax3.set_ylabel('Skill Improvement')
        ax3.set_title('Non-Linear Relationship: Engagement vs Progress')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
    else:
        ax3.text(0.5, 0.5, 'Insufficient data or model failed for polynomial scatter plot', horizontalalignment='center', verticalalignment='center', transform=ax3.transAxes)
        ax3.set_title('Non-Linear Relationship: Engagement vs Progress')
        ax3.axis('off')


    # 4. Feature importance (Random Forest)
    ax4 = axes[1, 1]
    if not feature_importance.empty:
        top_features = feature_importance.head(8)
        # Ensure 'feature' column is strings and 'importance' are numbers
        ax4.barh(top_features['feature'].astype(str), top_features['importance'].astype(float), color='skyblue')
        ax4.set_xlabel('Importance')
        ax4.set_title('Feature Importance (Random Forest)')
        ax4.invert_yaxis() # Most important at the top
    else:
        ax4.text(0.5, 0.5, 'Insufficient feature importance data', horizontalalignment='center', verticalalignment='center', transform=ax4.transAxes)
        ax4.set_title('Feature Importance (Random Forest)')
        ax4.axis('off')

    # 5. Mediating variable analysis (Correlations with skill_improvement)
    ax5 = axes[2, 0]
    if not extended_analytics.empty:
        mediator_cols_for_corr = [
            'visual_focus_score', 'engagement_consistency',
            'learning_efficiency', 'adaptability_score',
            'stress_resistance', 'avg_task_time', # Added avg_task_time here for consistency
            'skill_improvement'
        ]
        # Dropna on the specific columns needed for correlation calculation
        temp_df_mediator_corr = extended_analytics[mediator_cols_for_corr].dropna()
        if len(temp_df_mediator_corr) > 1:
            # Calculate correlations only with 'skill_improvement'
            mediator_corrs = temp_df_mediator_corr.corr()['skill_improvement'].drop('skill_improvement') # Exclude self-correlation
            mediator_corrs.plot(kind='bar', ax=ax5, color='lightgreen')
            ax5.set_title('Mediating Variable Correlations with Progress')
            ax5.set_xlabel('Mediating Variables')
            ax5.set_ylabel('Correlation with Progress')
            ax5.tick_params(axis='x', rotation=45)
            ax5.axhline(0, color='grey', linewidth=0.8) # Add a line at 0 for clarity
        else:
            ax5.text(0.5, 0.5, 'Insufficient data for mediating correlations', horizontalalignment='center', verticalalignment='center', transform=ax5.transAxes)
            ax5.set_title('Mediating Variable Correlations')
            ax5.axis('off')
    else:
        ax5.text(0.5, 0.5, 'Insufficient data for mediating correlations', horizontalalignment='center', verticalalignment='center', transform=ax5.transAxes)
        ax5.set_title('Mediating Variable Correlations')
        ax5.axis('off')


    # 6. Model comparison (R2 scores)
    ax6 = axes[2, 1]
    # Ensure all required R2 scores are available and not NaN
    linear_r2_val = np.nan # Initialize linear R2
    if not extended_analytics.empty and len(extended_analytics) > 1:
        valid_data_linear = extended_analytics[['avg_engagement', 'skill_improvement']].dropna()
        if len(valid_data_linear) > 1:
            try:
                # Use LinearRegression for consistent R2 calculation
                lin_model = LinearRegression()
                lin_model.fit(valid_data_linear['avg_engagement'].values.reshape(-1, 1), valid_data_linear['skill_improvement'].values)
                linear_r2_val = lin_model.score(valid_data_linear['avg_engagement'].values.reshape(-1, 1), valid_data_linear['skill_improvement'].values)
            except Exception:
                linear_r2_val = np.nan

    if not np.isnan(linear_r2_val) and 2 in engagement_poly and not np.isnan(engagement_poly[2]['r2']) and not np.isnan(rf_r2) and not np.isnan(dt_r2):
        models_performance = {
            'Linear': linear_r2_val,
            'Polynomial': engagement_poly[2]['r2'],
            'Random Forest': rf_r2,
            'Decision Tree': dt_r2
        }
        model_names = list(models_performance.keys())
        r2_scores = list(models_performance.values())
        bars = ax6.bar(model_names, r2_scores, color=['blue', 'green', 'orange', 'red'])
        ax6.set_ylabel('RÂ² Score')
        ax6.set_title('Model Comparison (RÂ² Score)')
        ax6.set_ylim(0, max(r2_scores) * 1.1)

        # Add values on bars
        for bar, score in zip(bars, r2_scores):
            ax6.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                     f'{score:.3f}', ha='center', va='bottom')
    else:
        ax6.text(0.5, 0.5, 'Insufficient data or metrics for model comparison', horizontalalignment='center', verticalalignment='center', transform=ax6.transAxes)
        ax6.set_title('Model Comparison')
        ax6.axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent suptitle overlap
    plt.show()
else:
    print("\nSkipping visualizations due to missing data or failed analyses.")


# ---
### 7. SALVAR RESULTADOS


print("\n7. SAVING ANALYSIS RESULTS")
print("=" * 60)

# Re-check overall conditions for saving
saving_possible = (
    not extended_analytics.empty and
    not feature_importance.empty and
    not np.isnan(rf_r2) and
    not np.isnan(dt_r2)
)

if saving_possible:
    # Save extended data
    extended_analytics.to_csv('extended_analytics_with_mediators.csv', index=False)
    print("   - extended_analytics_with_mediators.csv: Data with mediating variables")

    # Save correlation results
    if style_correlations or severity_correlations or comm_correlations:
        correlation_results_dfs = {
            'learning_style_correlations': style_correlations,
            'severity_correlations': severity_correlations,
            'communication_correlations': comm_correlations
        }
        for key, value in correlation_results_dfs.items():
            if value: # Check if the dictionary is not empty
                pd.DataFrame.from_dict(value, orient='index').to_csv(f'{key}.csv')
                print(f"   - {key}.csv: Profile correlation results")
            else:
                print(f"   Skipping saving {key}.csv as data is empty.")
    else:
        print("   Correlation results not available for saving.")

    # Save feature importance
    feature_importance.to_csv('feature_importance_random_forest.csv', index=False)
    print("   - feature_importance_random_forest.csv: Feature importance (Random Forest)")

    # Save model metrics
    linear_r2_val_save = np.nan
    linear_mse_val_save = np.nan

    if not extended_analytics.empty and len(extended_analytics) > 1:
        valid_data_linear_save = extended_analytics[['avg_engagement', 'skill_improvement']].dropna()
        if len(valid_data_linear_save) > 1:
            try:
                lin_model_save = LinearRegression()
                lin_model_save.fit(valid_data_linear_save['avg_engagement'].values.reshape(-1, 1), valid_data_linear_save['skill_improvement'].values)
                linear_r2_val_save = lin_model_save.score(valid_data_linear_save['avg_engagement'].values.reshape(-1, 1), valid_data_linear_save['skill_improvement'].values)
                linear_mse_val_save = mean_squared_error(valid_data_linear_save['skill_improvement'], lin_model_save.predict(valid_data_linear_save['avg_engagement'].values.reshape(-1, 1)))
            except Exception:
                linear_r2_val_save = np.nan
                linear_mse_val_save = np.nan

    # Only save if all core metrics are valid
    if (not np.isnan(linear_r2_val_save) and 2 in engagement_poly and not np.isnan(engagement_poly[2]['r2']) and
        not np.isnan(rf_r2) and not np.isnan(dt_r2)):

        model_metrics = pd.DataFrame({
            'Model': ['Linear', 'Polynomial_Degree_2', 'Random_Forest', 'Decision_Tree'],
            'R2_Score': [
                linear_r2_val_save,
                engagement_poly[2]['r2'],
                rf_r2,
                dt_r2
            ],
            'MSE': [
                linear_mse_val_save,
                engagement_poly[2]['mse'],
                rf_mse,
                dt_mse
            ]
        })
        model_metrics.to_csv('model_comparison_metrics.csv', index=False)
        print("   - model_comparison_metrics.csv: Comparative model metrics")
    else:
        print("   Skipping saving model_comparison_metrics.csv due to missing or NaN metrics.")

    print("âœ… Analysis completed and results saved!")

else:
    print("Skipping saving results due to missing data or failed analyses.")


# ---
### 8. SUMÃRIO EXECUTIVO DA ANÃLISE


print("\n8. EXECUTIVE SUMMARY OF ANALYSIS")
print("=" * 60)

# Re-evaluate conditions for printing summary
summary_possible = (
    not np.isnan(rf_r2) and
    not feature_importance.empty
)

if summary_possible:
    print("ðŸŽ¯ KEY FINDINGS:")
    print(f"1. Most effective model: Random Forest (RÂ² = {rf_r2:.4f})")

    if 'engagement_poly' in locals() and engagement_poly and 2 in engagement_poly and not np.isnan(engagement_poly[2]['r2']):
        print(f"2. Non-linear relationship detected: Engagement vs Progress (Polynomial Degree 2 RÂ² = {engagement_poly[2]['r2']:.4f})")
    else:
        print("2. Non-linear relationship analysis skipped due to missing data.")

    if not feature_importance.empty:
        print(f"3. Most important mediating variable (Random Forest): **{feature_importance.iloc[0]['feature']}**")
    else:
        print("3. Mediating variable importance not calculated.")


    print("\nðŸ” CORRELATIONS BY PROFILE:")
    if style_correlations:
        # Filter styles that actually had enough data for correlation calculation
        styles_with_valid_data = {k: v for k, v in style_correlations.items() if not np.isnan(v['engagement_corr'])}
        if styles_with_valid_data:
            # Sort by engagement correlation to find the "best" (highest positive correlation)
            best_style = max(styles_with_valid_data.items(), key=lambda x: x[1]['engagement_corr'])
            print(f"- Best engagement-progress correlation by style: **{best_style[0].capitalize()}** (r={best_style[1]['engagement_corr']:.3f})")
        else:
            print("- Correlation analysis by style with insufficient data.")
    else:
        print("- Correlation analysis by style not performed.")


    if severity_correlations:
        # Filter severities that actually had enough data for correlation calculation
        severities_with_valid_data = {k: v for k, v in severity_correlations.items() if not np.isnan(v['engagement_corr'])}
        if severities_with_valid_data:
            # Sort by engagement correlation to find the "best" (highest positive correlation)
            best_severity = max(severities_with_valid_data.items(), key=lambda x: x[1]['engagement_corr'])
            print(f"- Severity with best engagement-progress correlation: **{best_severity[0].capitalize()}** (r={best_severity[1]['engagement_corr']:.3f})")
        else:
            print("- Correlation analysis by severity with insufficient data.")
    else:
        print("- Correlation analysis by severity not performed.")


    print("\nðŸ“Š IMPLICATIONS FOR COMPUTATIONAL SCAFFOLDING:")
    print("1. **Personalization by learning style** is crucial for optimizing progress.")
    print("2. The inclusion of **mediating variables**, such as visual focus and stress resistance, significantly enriches the predictive capability of models, revealing underlying mechanisms.")
    print("3. **Non-linear relationships** suggest the need for adaptive algorithms that can dynamically adjust scaffolding strategies, rather than simplistic linear approaches.")
    if 'stress_resistance' in feature_importance['feature'].values:
        stress_rank = feature_importance[feature_importance['feature'] == 'stress_resistance'].index[0] # Get row index
        if feature_importance.loc[stress_rank, 'importance'] > 0.05: # Arbitrary threshold for "important"
            print("4. **Stress resistance** is an important mediating factor, indicating that stress management is crucial for academic performance.")
        else:
            print("4. Stress resistance is a mediating factor to consider, though its importance may vary.")
    else:
        print("4. The importance of stress resistance as a mediator was not calculated or the variable is not present.")


    print("\nâœ¨ ADVANCED ANALYSIS COMPLETED!")
    print("The results provide deep insights for the hybrid computational scaffolding model for students with ASD.")
else:
    print("\nSkipping executive summary due to missing data or failed analyses.")
